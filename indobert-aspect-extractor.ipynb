{"cells":[{"cell_type":"markdown","metadata":{},"source":["Identitas Kelompok  \n","Nico Samuelson / C14210017  \n","Darrell Cornelius Rivaldo / C14210025  \n","Nicholas Gunawan / C14210099  \n","Michael Adi Pratama / C14210016"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-26T02:05:05.407592Z","iopub.status.busy":"2024-06-26T02:05:05.407222Z","iopub.status.idle":"2024-06-26T02:05:34.781729Z","shell.execute_reply":"2024-06-26T02:05:34.780597Z","shell.execute_reply.started":"2024-06-26T02:05:05.407558Z"},"trusted":true},"outputs":[],"source":["import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","\n","import numpy as np\n","import pandas as pd\n","import re\n","\n","from datasets import Dataset\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n","\n","from sklearn.metrics import f1_score"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T02:05:34.784189Z","iopub.status.busy":"2024-06-26T02:05:34.783568Z","iopub.status.idle":"2024-06-26T02:05:34.789119Z","shell.execute_reply":"2024-06-26T02:05:34.787937Z","shell.execute_reply.started":"2024-06-26T02:05:34.784157Z"},"trusted":true},"outputs":[],"source":["seed = 123"]},{"cell_type":"markdown","metadata":{},"source":["# Reading Dataset"]},{"cell_type":"markdown","metadata":{},"source":["membaca data, data dishuffle, kemudian di split menjadi 2 satu untuk data training dan satu untuk data validasi"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T02:05:34.791200Z","iopub.status.busy":"2024-06-26T02:05:34.790564Z","iopub.status.idle":"2024-06-26T02:05:35.021213Z","shell.execute_reply":"2024-06-26T02:05:35.020310Z","shell.execute_reply.started":"2024-06-26T02:05:34.791164Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['review', 'rating', 'pelayanan', 'pengiriman', 'barang', '__index_level_0__'],\n","    num_rows: 2894\n","})"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["dataset = pd.read_csv('/kaggle/input/absa-aspect/aspect.csv')\n","dataset = dataset.sample(frac=1, random_state=seed) # shuffle\n","\n","train_frac = 0.8\n","train_size = int(train_frac * dataset.shape[0])\n","train = Dataset.from_pandas(dataset.iloc[:train_size])\n","val = Dataset.from_pandas(dataset.iloc[train_size:])\n","train"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocessing Dataset"]},{"cell_type":"markdown","metadata":{},"source":["1. membuat dictionary label2id dan id2label\n","1. melakukan preprocessing dataset\n","    - mengubah semua huruf pada review menjadi lowercase\n","    - menghapus semua tanda baca pada review\n","    - menghapus semua emoji pada review\n","    - melakukan multi hot encoding pada label"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T02:05:35.023582Z","iopub.status.busy":"2024-06-26T02:05:35.023285Z","iopub.status.idle":"2024-06-26T02:05:36.404343Z","shell.execute_reply":"2024-06-26T02:05:36.403501Z","shell.execute_reply.started":"2024-06-26T02:05:35.023558Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4463f47753ce472d90df9fcabb3ae5bc","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"706a1ff4bcb34050a8a11b7eaffeaab9","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"09d0171fc02c4ca19efeb21afb12c22a","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/234k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"96c58e2219764d88af36dd7b5413471c","version_major":2,"version_minor":0},"text/plain":["added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b52c48aa92a745ff89dbe22c6186db92","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"433f04f3ff194ca28d9fedc10b893356","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2894 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b37d54eba9ea48fab54f5e76b186f6f4","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/724 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n","    num_rows: 2894\n","})\n","Dataset({\n","    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n","    num_rows: 724\n","})\n"]}],"source":["# Define the labels and mappings\n","labels = [label for label in dataset.columns if label not in ('review', 'rating', '__index_level_0__')]\n","id2label = {idx: label for idx, label in enumerate(labels)}\n","label2id = {label: idx for idx, label in enumerate(labels)}\n","\n","# Initialize the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"indolem/indobert-base-uncased\")\n","\n","def clean_text(texts):\n","    cleaned_text = []\n","    \n","    for text in texts:\n","        \n","        text = text.lower()\n","\n","        text = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n","\n","        punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n","        for p in punctuations:\n","            text = text.replace(p,'') #Removing punctuations\n","\n","        emoji_pattern = re.compile(\"[\"\n","                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                               u\"\\U00002702-\\U000027B0\"\n","                               u\"\\U000024C2-\\U0001F251\"\n","                               \"]+\", flags=re.UNICODE)\n","        text = emoji_pattern.sub(r'', text) #Removing emojis\n","        cleaned_text.append(text)\n","    \n","    return cleaned_text\n","\n","# Define the preprocess function\n","def preprocess_data(examples):\n","    # Take a batch of texts\n","    text = clean_text(examples[\"review\"])\n","    # Encode them\n","    encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n","    # Add labels\n","    labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n","    # Create numpy array of shape (batch_size, num_labels)\n","    labels_matrix = np.zeros((len(text), len(labels)))\n","    # Fill numpy array\n","    for idx, label in enumerate(labels):\n","        labels_matrix[:, idx] = labels_batch[label]\n","\n","    encoding[\"labels\"] = labels_matrix.tolist()\n","  \n","    return encoding\n","\n","# Process the datasets\n","processed_train = train.map(preprocess_data, batched=True, remove_columns=train.column_names)\n","processed_val = val.map(preprocess_data, batched=True, remove_columns=val.column_names)\n","\n","# Output the shapes of the processed datasets for verification\n","print(processed_train)\n","print(processed_val)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T02:05:36.405777Z","iopub.status.busy":"2024-06-26T02:05:36.405493Z","iopub.status.idle":"2024-06-26T02:05:36.410640Z","shell.execute_reply":"2024-06-26T02:05:36.409811Z","shell.execute_reply.started":"2024-06-26T02:05:36.405753Z"},"trusted":true},"outputs":[],"source":["processed_train.set_format('torch')\n","processed_val.set_format('torch')"]},{"cell_type":"markdown","metadata":{},"source":["# Inisialisasi Model IndoBERT"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T02:05:36.411997Z","iopub.status.busy":"2024-06-26T02:05:36.411765Z","iopub.status.idle":"2024-06-26T02:05:39.927877Z","shell.execute_reply":"2024-06-26T02:05:39.926965Z","shell.execute_reply.started":"2024-06-26T02:05:36.411976Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4786db87f2fa43b9978313b066d703d7","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/445M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indolem/indobert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Load pre-trained model with a classification head\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    \"indolem/indobert-base-uncased\", \n","    problem_type='multi_label_classification',\n","    num_labels=len(labels),\n","    id2label=id2label,\n","    label2id=label2id\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T02:05:39.929743Z","iopub.status.busy":"2024-06-26T02:05:39.929373Z","iopub.status.idle":"2024-06-26T02:05:39.933883Z","shell.execute_reply":"2024-06-26T02:05:39.932983Z","shell.execute_reply.started":"2024-06-26T02:05:39.929699Z"},"trusted":true},"outputs":[],"source":["batch_size = 8\n","metric_name = \"f1\""]},{"cell_type":"markdown","metadata":{},"source":["Model IndoBERT akan ditrain sebanyak 10 epoch dengan learning rate 0.00002 dan weight decay 0.01. Model terakhir yang akan diambil adalah model pada epoch dengan metric micro F1 tertinggi"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T02:05:39.935453Z","iopub.status.busy":"2024-06-26T02:05:39.935108Z","iopub.status.idle":"2024-06-26T02:05:40.067617Z","shell.execute_reply":"2024-06-26T02:05:40.066638Z","shell.execute_reply.started":"2024-06-26T02:05:39.935422Z"},"trusted":true},"outputs":[],"source":["args = TrainingArguments(\n","    f\"bert-finetuned-aspect-extractor\",\n","    eval_strategy = \"epoch\",\n","    save_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=10,\n","    weight_decay=0.01,\n","    save_total_limit=1,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=metric_name,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["selain nilai micro f1 selama training, micro ROC AUC dan akurasi juga dihitung untuk tiap epochnya terhadap data validasi"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T02:05:40.069183Z","iopub.status.busy":"2024-06-26T02:05:40.068882Z","iopub.status.idle":"2024-06-26T02:05:40.077551Z","shell.execute_reply":"2024-06-26T02:05:40.076669Z","shell.execute_reply.started":"2024-06-26T02:05:40.069156Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n","from transformers import EvalPrediction\n","import torch\n","    \n","# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n","def multi_label_metrics(predictions, labels, threshold=0.5):\n","    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n","    sigmoid = torch.nn.Sigmoid()\n","    probs = sigmoid(torch.Tensor(predictions))\n","    # next, use threshold to turn them into integer predictions\n","    y_pred = np.zeros(probs.shape)\n","    y_pred[np.where(probs >= threshold)] = 1\n","    # finally, compute metrics\n","    y_true = labels\n","    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n","    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n","    accuracy = accuracy_score(y_true, y_pred)\n","    # return as dictionary\n","    metrics = {'f1': f1_micro_average,\n","               'roc_auc': roc_auc,\n","               'accuracy': accuracy}\n","    return metrics\n","\n","def compute_metrics(p: EvalPrediction):\n","    preds = p.predictions[0] if isinstance(p.predictions, \n","            tuple) else p.predictions\n","    result = multi_label_metrics(\n","        predictions=preds, \n","        labels=p.label_ids)\n","    return result"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T02:05:40.081252Z","iopub.status.busy":"2024-06-26T02:05:40.080491Z","iopub.status.idle":"2024-06-26T02:16:32.722205Z","shell.execute_reply":"2024-06-26T02:16:32.721215Z","shell.execute_reply.started":"2024-06-26T02:05:40.081219Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["wandb version 0.17.3 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.17.0"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240626_020625-vz4jfcov</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/tokped_sentiment_analysis/huggingface/runs/vz4jfcov' target=\"_blank\">bert-finetuned-aspect-extractor</a></strong> to <a href='https://wandb.ai/tokped_sentiment_analysis/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/tokped_sentiment_analysis/huggingface' target=\"_blank\">https://wandb.ai/tokped_sentiment_analysis/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/tokped_sentiment_analysis/huggingface/runs/vz4jfcov' target=\"_blank\">https://wandb.ai/tokped_sentiment_analysis/huggingface/runs/vz4jfcov</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1810' max='1810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1810/1810 09:45, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>F1</th>\n","      <th>Roc Auc</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.327131</td>\n","      <td>0.866953</td>\n","      <td>0.856108</td>\n","      <td>0.691989</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.267116</td>\n","      <td>0.910790</td>\n","      <td>0.901631</td>\n","      <td>0.790055</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.359200</td>\n","      <td>0.273375</td>\n","      <td>0.902131</td>\n","      <td>0.896312</td>\n","      <td>0.777624</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.359200</td>\n","      <td>0.253079</td>\n","      <td>0.917247</td>\n","      <td>0.912580</td>\n","      <td>0.814917</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.359200</td>\n","      <td>0.256428</td>\n","      <td>0.917962</td>\n","      <td>0.911992</td>\n","      <td>0.808011</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.189800</td>\n","      <td>0.272159</td>\n","      <td>0.914608</td>\n","      <td>0.909021</td>\n","      <td>0.802486</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.189800</td>\n","      <td>0.265638</td>\n","      <td>0.924935</td>\n","      <td>0.919330</td>\n","      <td>0.825967</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.189800</td>\n","      <td>0.273252</td>\n","      <td>0.926307</td>\n","      <td>0.919784</td>\n","      <td>0.827348</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.123400</td>\n","      <td>0.276513</td>\n","      <td>0.921807</td>\n","      <td>0.917026</td>\n","      <td>0.819061</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.123400</td>\n","      <td>0.279330</td>\n","      <td>0.923608</td>\n","      <td>0.917972</td>\n","      <td>0.820442</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/plain":["TrainOutput(global_step=1810, training_loss=0.2014253943005978, metrics={'train_runtime': 650.8218, 'train_samples_per_second': 44.467, 'train_steps_per_second': 2.781, 'total_flos': 1903625577262080.0, 'train_loss': 0.2014253943005978, 'epoch': 10.0})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=processed_train,\n","    eval_dataset=processed_val,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","trainer.train()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T02:16:48.447616Z","iopub.status.busy":"2024-06-26T02:16:48.447232Z","iopub.status.idle":"2024-06-26T02:16:52.606643Z","shell.execute_reply":"2024-06-26T02:16:52.605555Z","shell.execute_reply.started":"2024-06-26T02:16:48.447584Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='46' max='46' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [46/46 00:03]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_loss': 0.2732519507408142,\n"," 'eval_f1': 0.9263067694944302,\n"," 'eval_roc_auc': 0.9197844436782491,\n"," 'eval_accuracy': 0.8273480662983426,\n"," 'eval_runtime': 4.1452,\n"," 'eval_samples_per_second': 174.659,\n"," 'eval_steps_per_second': 11.097,\n"," 'epoch': 10.0}"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["trainer.evaluate()"]},{"cell_type":"markdown","metadata":{},"source":["Jika melihat dari micro f1 dan akurasi model IndoBERT secara berturut-turut 0.9263 dan 0.8273, menunjukkan bahwa model IndoBERT mengungguli kedua model lainnya yaitu MultilingualBERT dan Roberta Indonesia."]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T02:25:29.248220Z","iopub.status.busy":"2024-06-26T02:25:29.247601Z","iopub.status.idle":"2024-06-26T02:25:29.275695Z","shell.execute_reply":"2024-06-26T02:25:29.274801Z","shell.execute_reply.started":"2024-06-26T02:25:29.248187Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['pengiriman', 'barang']\n"]}],"source":["text = \"Packaging asal-asalan\"\n","\n","encoding = tokenizer(text, return_tensors=\"pt\")\n","encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n","\n","outputs = trainer.model(**encoding)\n","logits = outputs.logits\n","\n","# apply sigmoid + threshold\n","sigmoid = torch.nn.Sigmoid()\n","probs = sigmoid(logits.squeeze().cpu())\n","predictions = np.zeros(probs.shape)\n","predictions[np.where(probs >= 0.5)] = 1\n","# turn predicted id's into actual label names\n","predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n","print(predicted_labels)"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T02:27:07.732910Z","iopub.status.busy":"2024-06-26T02:27:07.732292Z","iopub.status.idle":"2024-06-26T02:27:08.300685Z","shell.execute_reply":"2024-06-26T02:27:08.299669Z","shell.execute_reply.started":"2024-06-26T02:27:07.732878Z"},"trusted":true},"outputs":[],"source":["trainer.model.save_pretrained('indobert-aspect-extractor')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5281262,"sourceId":8785064,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
