{"cells":[{"cell_type":"markdown","metadata":{},"source":["Identitas Kelompok  \n","Nico Samuelson / C14210017  \n","Darrell Cornelius Rivaldo / C14210025  \n","Nicholas Gunawan / C14210099  \n","Michael Adi Pratama / C14210016"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-26T16:45:10.556095Z","iopub.status.busy":"2024-06-26T16:45:10.555367Z","iopub.status.idle":"2024-06-26T16:45:42.201714Z","shell.execute_reply":"2024-06-26T16:45:42.200935Z","shell.execute_reply.started":"2024-06-26T16:45:10.556059Z"},"trusted":true},"outputs":[],"source":["import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","\n","import numpy as np\n","import pandas as pd\n","import re\n","\n","from datasets import load_from_disk\n","from datasets import Dataset\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n","\n","from sklearn.metrics import f1_score"]},{"cell_type":"markdown","metadata":{},"source":["# PREPROCESSING DATASET"]},{"cell_type":"markdown","metadata":{},"source":["Proses Text Cleaning\n","1. Lowercase semua text\n","2. Replace semua non alphabetic karakter menjadi spasi\n","3. Menghilangkan semua tanda baca\n","4. Menghilangkan semua emoji"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def clean_text(texts):\n","    cleaned_text = []\n","    \n","    for text in texts:\n","        \n","        text = text.lower()\n","\n","        text = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n","\n","        punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n","        for p in punctuations:\n","            text = text.replace(p,'') #Removing punctuations\n","\n","        emoji_pattern = re.compile(\"[\"\n","                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                               u\"\\U00002702-\\U000027B0\"\n","                               u\"\\U000024C2-\\U0001F251\"\n","                               \"]+\", flags=re.UNICODE)\n","        text = emoji_pattern.sub(r'', text) #Removing emojis\n","        cleaned_text.append(text)\n","    \n","    return cleaned_text"]},{"cell_type":"markdown","metadata":{},"source":["Preprocess Teks\n","1. Tokenize and clean teks\n","2. One hot label"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Preprocess function\n","def preprocess_function(examples):\n","    combined_texts = [aspect_token + aspect + review_token + review for aspect, review in zip(examples[\"variable\"], examples[\"review\"])]\n","    encoding =  tokenizer(\n","        clean_text(examples[\"review\"]), \n","        padding=\"max_length\", \n","        truncation=True, \n","        max_length=128\n","    )\n","    \n","\n","    labels_matrix = np.zeros((len(examples['review']), 3))\n","    \n","    for i, label in enumerate(examples[\"value\"]):\n","        labels_matrix[i, int(label)] = 1\n","\n","        encoding[\"labels\"] = labels_matrix.tolist()\n","  \n","    return encoding"]},{"cell_type":"markdown","metadata":{},"source":["# INDOBERT\n","1. Import IndoBERT tokenizer dari Hugging Face\n","2. Menambahkan 2 token special untuk memisahkan aspek dan review dari input"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"indolem/indobert-base-uncased\")\n","review_token = '[REVIEW]'\n","aspect_token = '[ASPECT]'\n","special_tokens_dict = {'additional_special_tokens': [review_token, aspect_token]}\n","num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Preprocess function\n","def preprocess_function(examples):\n","    combined_texts = [aspect_token + aspect + review_token + review for aspect, review in zip(examples[\"variable\"], examples[\"review\"])]\n","    encoding =  tokenizer(\n","        clean_text(examples[\"review\"]), \n","        padding=\"max_length\", \n","        truncation=True, \n","        max_length=128\n","    )\n","\n","    labels_matrix = np.zeros((len(examples['review']), 3))\n","    \n","    for i, label in enumerate(examples[\"value\"]):\n","        labels_matrix[i, int(label)] = 1\n","\n","        encoding[\"labels\"] = labels_matrix.tolist()\n","  \n","    return encoding"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_df = pd.read_csv('/kaggle/input/bert-absa-dataset/out.csv').iloc[:4612, :]\n","test_df = pd.read_csv('/kaggle/input/bert-absa-dataset/out.csv').iloc[4612:, :]\n","\n","# Convert your data to Hugging Face Dataset\n","train_dataset = Dataset.from_pandas(train_df)\n","test_dataset = Dataset.from_pandas(test_df)\n","\n","tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n","tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n","\n","tokenized_train_dataset = tokenized_train_dataset.remove_columns([\"review\", \"rating\", \"variable\", \"value\"])\n","tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"review\", \"rating\", \"variable\", \"value\"])"]},{"cell_type":"markdown","metadata":{},"source":["1. Fine tune model IndoBERT menggunakan AutoModelForSequenceClassification untuk melakukan tugas klasifikasi\n","2. Mengubah ukuran dari Embedding layer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load pre-trained model with a classification head\n","model = AutoModelForSequenceClassification.from_pretrained(\"indolem/indobert-base-uncased\", num_labels=3)\n","model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"markdown","metadata":{},"source":["Fungsi untuk menghitung akurasi dari setiap epoch"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n","from transformers import EvalPrediction\n","import torch\n","    \n","def accuracy_metric(predictions, labels, threshold=0.5):\n","    y_true = labels\n","    \n","    y_pred_indices = np.argmax(predictions, axis=1)\n","    y_pred_one_hot = np.zeros_like(predictions)\n","    y_pred_one_hot[np.arange(predictions.shape[0]), y_pred_indices] = 1\n","    \n","    accuracy = accuracy_score(y_true, y_pred_one_hot)\n","    # return as dictionary\n","    metrics = {'accuracy': accuracy}\n","    return metrics\n","\n","def compute_metrics(p: EvalPrediction):\n","    preds = p.predictions[0] if isinstance(p.predictions, \n","            tuple) else p.predictions\n","    result = accuracy_metric(\n","        predictions=preds, \n","        labels=p.label_ids)\n","    return result"]},{"cell_type":"markdown","metadata":{},"source":["Training setup:\n","1. Learning rate 2e-5\n","2. Batch size 16\n","3. Weight decay 0.01\n","4. Ditrain sebanyak 3 epoch"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import TrainingArguments, Trainer\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    eval_strategy = \"epoch\",\n","    save_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=3,\n","    load_best_model_at_end=True,\n","    weight_decay=0.01,\n",")\n","\n","# Define trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train_dataset,\n","    eval_dataset=tokenized_test_dataset,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","\n","# Train the model\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-25T17:08:23.822545Z","iopub.status.busy":"2024-06-25T17:08:23.822171Z","iopub.status.idle":"2024-06-25T17:08:24.539454Z","shell.execute_reply":"2024-06-25T17:08:24.538313Z","shell.execute_reply.started":"2024-06-25T17:08:23.822512Z"},"trusted":true},"outputs":[],"source":["model.save_pretrained(\"indobert-absa-model-final\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T16:51:00.323604Z","iopub.status.busy":"2024-06-26T16:51:00.323049Z","iopub.status.idle":"2024-06-26T16:51:05.941264Z","shell.execute_reply":"2024-06-26T16:51:05.940207Z","shell.execute_reply.started":"2024-06-26T16:51:00.323569Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [36/36 00:05]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.23248052597045898, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 5.6028, 'eval_samples_per_second': 205.612, 'eval_steps_per_second': 6.425, 'epoch': 3.0}\n"]}],"source":["results = trainer.evaluate(tokenized_test_dataset)\n","print(results)"]},{"cell_type":"markdown","metadata":{},"source":["Fungsi untuk melakukan inference terhadap teks"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-25T17:08:17.833823Z","iopub.status.busy":"2024-06-25T17:08:17.833043Z","iopub.status.idle":"2024-06-25T17:08:17.871069Z","shell.execute_reply":"2024-06-25T17:08:17.870192Z","shell.execute_reply.started":"2024-06-25T17:08:17.833792Z"},"trusted":true},"outputs":[],"source":["import torch\n","\n","# Example test sample\n","test_aspect = \"barang\"\n","test_review = \"lumayan\"\n","\n","# Preprocess the test sample\n","def preprocess_single_sample(aspect, review):\n","    combined_texts = aspect_token + aspect + review_token + review\n","    print(len(combined_texts))\n","    encoding = tokenizer(\n","        combined_texts, \n","        padding=\"max_length\", \n","        truncation=True, \n","        max_length=128,\n","        return_tensors=\"pt\"\n","    )\n","    \n","    return encoding\n","\n","# Preprocess the sample\n","encoding = preprocess_single_sample(test_aspect, test_review)\n","\n","# Move inputs to the same device as the model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","inputs = {key: value.to(device) for key, value in encoding.items()}\n","model.to(device)\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Run inference\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","\n","# Get predicted label\n","logits = outputs.logits\n","# print(logits.shape)\n","predicted_class_id = torch.argmax(logits, dim=1).item()\n","\n","# Map class id to label (Assuming 0: Negative, 1: Neutral, 2: Positive)\n","label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n","predicted_label = label_map[predicted_class_id]\n","\n","print(f\"Aspect: {test_aspect}\")\n","print(f\"Review: {test_review}\")\n","print(f\"Predicted Sentiment: {predicted_label}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# BERT MULTILINGUAL\n","1. Import IndoBERT tokenizer dari Hugging Face\n","2. Menambahkan 2 token special untuk memisahkan aspek dan review dari input"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T16:51:10.235561Z","iopub.status.busy":"2024-06-26T16:51:10.234893Z","iopub.status.idle":"2024-06-26T16:51:14.569339Z","shell.execute_reply":"2024-06-26T16:51:14.568134Z","shell.execute_reply.started":"2024-06-26T16:51:10.235528Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c23d3b2912dc4048a96bb08f9bef3c3b","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"24e8a2d5767f4048bab00c9723dca231","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d146aac44cb54c44a2700966b454a2e6","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"58009c9d9f024c538445d34cf600547d","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f330c9e8254c421bb69be78a4d8f9975","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/4612 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e193d15a53464393bb2bc2cfad74216f","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1152 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n","review_token = '[REVIEW]'\n","aspect_token = '[ASPECT]'\n","special_tokens_dict = {'additional_special_tokens': [review_token, aspect_token]}\n","num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Preprocess function\n","def preprocess_function(examples):\n","    combined_texts = [aspect_token + aspect + review_token + review for aspect, review in zip(examples[\"variable\"], examples[\"review\"])]\n","    encoding =  tokenizer(\n","        clean_text(examples[\"review\"]), \n","        padding=\"max_length\", \n","        truncation=True, \n","        max_length=128\n","    )\n","    \n","\n","    labels_matrix = np.zeros((len(examples['review']), 3))\n","    \n","    for i, label in enumerate(examples[\"value\"]):\n","        labels_matrix[i, int(label)] = 1\n","\n","        encoding[\"labels\"] = labels_matrix.tolist()\n","  \n","    return encoding"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_df = pd.read_csv('/kaggle/input/bert-absa-dataset/out.csv').iloc[:4612, :]\n","test_df = pd.read_csv('/kaggle/input/bert-absa-dataset/out.csv').iloc[4612:, :]\n","\n","# Convert your data to Hugging Face Dataset\n","train_dataset = Dataset.from_pandas(train_df)\n","test_dataset = Dataset.from_pandas(test_df)\n","\n","tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n","tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n","\n","tokenized_train_dataset = tokenized_train_dataset.remove_columns([\"review\", \"rating\", \"variable\", \"value\"])\n","tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"review\", \"rating\", \"variable\", \"value\"])"]},{"cell_type":"markdown","metadata":{},"source":["1. Fine tune model BERT Multilingual menggunakan AutoModelForSequenceClassification untuk melakukan tugas klasifikasi\n","2. Mengubah ukuran dari Embedding layer"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T16:51:16.089659Z","iopub.status.busy":"2024-06-26T16:51:16.088863Z","iopub.status.idle":"2024-06-26T16:51:43.266659Z","shell.execute_reply":"2024-06-26T16:51:43.265386Z","shell.execute_reply.started":"2024-06-26T16:51:16.089626Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"922b3780923b4d4db4a97696e8d9852e","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["Embedding(119549, 768)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Load pre-trained model with a classification head\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=3)\n","model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"markdown","metadata":{},"source":["Training setup:\n","1. Learning rate 2e-5\n","2. Batch size 16\n","3. Weight decay 0.01\n","4. Ditrain sebanyak 3 epoch"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T06:33:32.865065Z","iopub.status.busy":"2024-06-26T06:33:32.864676Z","iopub.status.idle":"2024-06-26T06:39:12.67008Z","shell.execute_reply":"2024-06-26T06:39:12.669219Z","shell.execute_reply.started":"2024-06-26T06:33:32.865036Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["wandb version 0.17.3 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.17.0"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240626_063412-7p4xxxqb</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/tokped_sentiment_analysis/huggingface/runs/7p4xxxqb' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/tokped_sentiment_analysis/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/tokped_sentiment_analysis/huggingface' target=\"_blank\">https://wandb.ai/tokped_sentiment_analysis/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/tokped_sentiment_analysis/huggingface/runs/7p4xxxqb' target=\"_blank\">https://wandb.ai/tokped_sentiment_analysis/huggingface/runs/7p4xxxqb</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='435' max='435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [435/435 04:38, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.276519</td>\n","      <td>0.820312</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.245479</td>\n","      <td>0.848958</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.249207</td>\n","      <td>0.853299</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/plain":["TrainOutput(global_step=435, training_loss=0.23655486709770115, metrics={'train_runtime': 337.3014, 'train_samples_per_second': 41.02, 'train_steps_per_second': 1.29, 'total_flos': 910109311921152.0, 'train_loss': 0.23655486709770115, 'epoch': 3.0})"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import TrainingArguments, Trainer\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    eval_strategy = \"epoch\",\n","    save_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=3,\n","    load_best_model_at_end=True,\n","    weight_decay=0.01,\n",")\n","\n","# Define trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train_dataset,\n","    eval_dataset=tokenized_test_dataset,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","\n","# Train the model\n","trainer.train()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T06:39:52.114154Z","iopub.status.busy":"2024-06-26T06:39:52.11352Z","iopub.status.idle":"2024-06-26T06:39:58.824998Z","shell.execute_reply":"2024-06-26T06:39:58.82394Z","shell.execute_reply.started":"2024-06-26T06:39:52.114121Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [36/36 00:06]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.2454787641763687, 'eval_accuracy': 0.8489583333333334, 'eval_runtime': 6.6989, 'eval_samples_per_second': 171.969, 'eval_steps_per_second': 5.374, 'epoch': 3.0}\n"]}],"source":["results = trainer.evaluate(tokenized_test_dataset)\n","print(results)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T06:40:21.479834Z","iopub.status.busy":"2024-06-26T06:40:21.479377Z","iopub.status.idle":"2024-06-26T06:40:23.050261Z","shell.execute_reply":"2024-06-26T06:40:23.049189Z","shell.execute_reply.started":"2024-06-26T06:40:21.479794Z"},"trusted":true},"outputs":[],"source":["model.save_pretrained(\"multilingual-absa-model-final\")"]},{"cell_type":"markdown","metadata":{},"source":["# Indo ROBERTA\n","1. Import IndoBERT tokenizer dari Hugging Face\n","2. Menambahkan 2 token special untuk memisahkan aspek dan review dari input"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T06:40:56.598147Z","iopub.status.busy":"2024-06-26T06:40:56.597298Z","iopub.status.idle":"2024-06-26T06:41:01.609839Z","shell.execute_reply":"2024-06-26T06:41:01.60858Z","shell.execute_reply.started":"2024-06-26T06:40:56.598117Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"303c90f1f2ce4d14a34c0909479b3222","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"110f3a5d400b46b49b1dc81be509a9d1","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"49d68d30fbf043b8b5cf3d74926e2edf","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/926k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ce525a32cfce41b1b9a6e95302d61ee8","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/468k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ed969c4d31244779ae7328d98c510607","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/4612 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fccc0199c9944f5182ffb70744e4bfa3","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1152 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"cahya/roberta-base-indonesian-522M\")\n","review_token = '[REVIEW]'\n","aspect_token = '[ASPECT]'\n","special_tokens_dict = {'additional_special_tokens': [review_token, aspect_token]}\n","num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Preprocess function\n","def preprocess_function(examples):\n","    combined_texts = [aspect_token + aspect + review_token + review for aspect, review in zip(examples[\"variable\"], examples[\"review\"])]\n","    encoding =  tokenizer(\n","        clean_text(examples[\"review\"]), \n","        padding=\"max_length\", \n","        truncation=True, \n","        max_length=128\n","    )\n","    \n","\n","    labels_matrix = np.zeros((len(examples['review']), 3))\n","    \n","    for i, label in enumerate(examples[\"value\"]):\n","        labels_matrix[i, int(label)] = 1\n","\n","        encoding[\"labels\"] = labels_matrix.tolist()\n","  \n","    return encoding"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_df = pd.read_csv('/kaggle/input/bert-absa-dataset/out.csv').iloc[:4612, :]\n","test_df = pd.read_csv('/kaggle/input/bert-absa-dataset/out.csv').iloc[4612:, :]\n","\n","# Convert your data to Hugging Face Dataset\n","train_dataset = Dataset.from_pandas(train_df)\n","test_dataset = Dataset.from_pandas(test_df)\n","\n","tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n","tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n","\n","tokenized_train_dataset = tokenized_train_dataset.remove_columns([\"review\", \"rating\", \"variable\", \"value\"])\n","tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"review\", \"rating\", \"variable\", \"value\"])"]},{"cell_type":"markdown","metadata":{},"source":["1. Fine tune model Indo ROBERTA menggunakan AutoModelForSequenceClassification untuk melakukan tugas klasifikasi\n","2. Mengubah ukuran dari Embedding layer"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T06:41:30.098494Z","iopub.status.busy":"2024-06-26T06:41:30.098115Z","iopub.status.idle":"2024-06-26T06:41:51.844853Z","shell.execute_reply":"2024-06-26T06:41:51.843549Z","shell.execute_reply.started":"2024-06-26T06:41:30.098465Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b58dd4b8a8264dcabda31c3501721ffb","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/507M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cahya/roberta-base-indonesian-522M and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["Embedding(52002, 768)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# Load pre-trained model with a classification head\n","model = AutoModelForSequenceClassification.from_pretrained(\"cahya/roberta-base-indonesian-522M\", num_labels=3)\n","model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"markdown","metadata":{},"source":["Training setup:\n","1. Learning rate 2e-5\n","2. Batch size 16\n","3. Weight decay 0.01\n","4. Ditrain sebanyak 3 epoch"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T06:42:00.483557Z","iopub.status.busy":"2024-06-26T06:42:00.483122Z","iopub.status.idle":"2024-06-26T06:45:50.526215Z","shell.execute_reply":"2024-06-26T06:45:50.525096Z","shell.execute_reply.started":"2024-06-26T06:42:00.483514Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='435' max='435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [435/435 03:48, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.258609</td>\n","      <td>0.843750</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.243934</td>\n","      <td>0.845486</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.246464</td>\n","      <td>0.845486</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/plain":["TrainOutput(global_step=435, training_loss=0.20075331063106142, metrics={'train_runtime': 229.1889, 'train_samples_per_second': 60.369, 'train_steps_per_second': 1.898, 'total_flos': 910109311921152.0, 'train_loss': 0.20075331063106142, 'epoch': 3.0})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import TrainingArguments, Trainer\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    eval_strategy = \"epoch\",\n","    save_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=3,\n","    load_best_model_at_end=True,\n","    weight_decay=0.01,\n",")\n","\n","# Define trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train_dataset,\n","    eval_dataset=tokenized_test_dataset,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","\n","# Train the model\n","trainer.train()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T06:45:50.528312Z","iopub.status.busy":"2024-06-26T06:45:50.527944Z","iopub.status.idle":"2024-06-26T06:45:56.350346Z","shell.execute_reply":"2024-06-26T06:45:56.349091Z","shell.execute_reply.started":"2024-06-26T06:45:50.528277Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [36/36 00:05]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.24393394589424133, 'eval_accuracy': 0.8454861111111112, 'eval_runtime': 5.8078, 'eval_samples_per_second': 198.355, 'eval_steps_per_second': 6.199, 'epoch': 3.0}\n"]}],"source":["results = trainer.evaluate(tokenized_test_dataset)\n","print(results)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-06-26T06:46:13.104868Z","iopub.status.busy":"2024-06-26T06:46:13.104433Z","iopub.status.idle":"2024-06-26T06:46:13.955553Z","shell.execute_reply":"2024-06-26T06:46:13.954423Z","shell.execute_reply.started":"2024-06-26T06:46:13.104836Z"},"trusted":true},"outputs":[],"source":["model.save_pretrained(\"roberta-absa-model-final\")"]},{"cell_type":"markdown","metadata":{},"source":["# KESIMPULAN\n","\n","Model IndoBERT memiliki performa yang lebih baik jika dibandingkan dengan model Multilingual BERT karena data kami yang multilingual jumlahnya sedikit (mayoritas bahasa Indonesia) sehingga IndoBERT memiliki keunggulan dimana model IndoBERT ditrain specifically pada data bahasa Indonesia"]},{"cell_type":"markdown","metadata":{},"source":["Sedangkan jika dibandingkan dengan Indo ROBERTA, kami memiliki beberapa asumsi seperti:\n","1. Perbedaan cara pelatihan BERT dan ROBERTA, dimana ROBERTA tidak melakukan Next Sentence Prediction (NSP) dalam pelatihannya, sehingga BERT dapat memahami relasi antar kata dengan lebih baik\n","2. Data yang digunakan untuk melatih model ROBERTA jauh lebih besar dari BERT sehingga ROBERTA lebih rentan mengalami overfitting"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5278420,"sourceId":8781255,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
