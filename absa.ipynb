{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nicost312/sentiment-analysis-tokopedia?scriptVersionId=185410100\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport numpy as np\nimport pandas as pd\nimport re\n\nfrom datasets import load_from_disk\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n\nfrom sklearn.metrics import f1_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-25T14:37:46.737689Z","iopub.execute_input":"2024-06-25T14:37:46.738565Z","iopub.status.idle":"2024-06-25T14:37:46.744142Z","shell.execute_reply.started":"2024-06-25T14:37:46.738522Z","shell.execute_reply":"2024-06-25T14:37:46.743072Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"indolem/indobert-base-uncased\")\nreview_token = '[REVIEW]'\naspect_token = '[ASPECT]'\nspecial_tokens_dict = {'additional_special_tokens': [review_token, aspect_token]}\nnum_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n\n\ndef clean_text(texts):\n    cleaned_text = []\n    \n    for text in texts:\n        \n        text = text.lower()\n\n        text = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n\n        punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n        for p in punctuations:\n            text = text.replace(p,'') #Removing punctuations\n\n        emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n        text = emoji_pattern.sub(r'', text) #Removing emojis\n        cleaned_text.append(text)\n    \n    return cleaned_text\n\n# Preprocess function\ndef preprocess_function(examples):\n    combined_texts = [aspect_token + aspect + review_token + review for aspect, review in zip(examples[\"variable\"], examples[\"review\"])]\n    encoding =  tokenizer(\n        clean_text(examples[\"review\"]), \n        padding=\"max_length\", \n        truncation=True, \n        max_length=128\n    )\n    \n\n    labels_matrix = np.zeros((len(examples['review']), 3))\n    \n#     print(labels_)\n    for i, label in enumerate(examples[\"value\"]):\n#         print(label)\n        labels_matrix[i, int(label)] = 1\n\n        encoding[\"labels\"] = labels_matrix.tolist()\n  \n    return encoding\n\ntrain_df = pd.read_csv('/kaggle/input/bert-absa-dataset/out.csv').iloc[:4612, :]\ntest_df = pd.read_csv('/kaggle/input/bert-absa-dataset/out.csv').iloc[4612:, :]\n\n# Convert your data to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\ntokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\ntokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n\ntokenized_train_dataset = tokenized_train_dataset.remove_columns([\"review\", \"rating\", \"variable\", \"value\"])\ntokenized_test_dataset = tokenized_test_dataset.remove_columns([\"review\", \"rating\", \"variable\", \"value\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-25T15:48:44.960978Z","iopub.execute_input":"2024-06-25T15:48:44.961813Z","iopub.status.idle":"2024-06-25T15:48:48.116897Z","shell.execute_reply.started":"2024-06-25T15:48:44.961779Z","shell.execute_reply":"2024-06-25T15:48:48.115873Z"},"trusted":true},"execution_count":110,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4612 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6137064a713f4508a0beca882fb332fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1152 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de6782a89fa34f839f58e31115c49c2f"}},"metadata":{}}]},{"cell_type":"code","source":"# Load pre-trained model with a classification head\nmodel = AutoModelForSequenceClassification.from_pretrained(\"indolem/indobert-base-uncased\", num_labels=3)\n# model = BertForSequenceClassification.from_pretrained(\n#     \"bert-base-multilingual-cased\", \n#     num_labels = 3,\n#     output_attentions = False, \n#     output_hidden_states = False\n# )\nmodel.resize_token_embeddings(len(tokenizer))\n# model = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T16:32:59.731713Z","iopub.execute_input":"2024-06-25T16:32:59.732083Z","iopub.status.idle":"2024-06-25T16:33:00.62543Z","shell.execute_reply.started":"2024-06-25T16:32:59.732054Z","shell.execute_reply":"2024-06-25T16:33:00.62441Z"},"trusted":true},"execution_count":124,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indolem/indobert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":124,"output_type":"execute_result","data":{"text/plain":"Embedding(31925, 768)"},"metadata":{}}]},{"cell_type":"code","source":"print(tokenized_train_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T15:48:52.040681Z","iopub.execute_input":"2024-06-25T15:48:52.041457Z","iopub.status.idle":"2024-06-25T15:48:52.047821Z","shell.execute_reply.started":"2024-06-25T15:48:52.041423Z","shell.execute_reply":"2024-06-25T15:48:52.046679Z"},"trusted":true},"execution_count":112,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n    num_rows: 4612\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"print(tokenized_train_dataset['labels'][0])","metadata":{"execution":{"iopub.status.busy":"2024-06-25T15:48:53.366432Z","iopub.execute_input":"2024-06-25T15:48:53.366814Z","iopub.status.idle":"2024-06-25T15:48:53.400738Z","shell.execute_reply.started":"2024-06-25T15:48:53.366784Z","shell.execute_reply":"2024-06-25T15:48:53.399793Z"},"trusted":true},"execution_count":113,"outputs":[{"name":"stdout","text":"[1.0, 0.0, 0.0]\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n)\n\n# Define trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_test_dataset,\n    tokenizer=tokenizer,\n)\n\n# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-25T16:33:03.926672Z","iopub.execute_input":"2024-06-25T16:33:03.927646Z","iopub.status.idle":"2024-06-25T16:36:24.104845Z","shell.execute_reply.started":"2024-06-25T16:33:03.927588Z","shell.execute_reply":"2024-06-25T16:36:24.103806Z"},"trusted":true},"execution_count":125,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='435' max='435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [435/435 03:18, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":125,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=435, training_loss=0.23567090966235632, metrics={'train_runtime': 199.1327, 'train_samples_per_second': 69.481, 'train_steps_per_second': 2.184, 'total_flos': 910109311921152.0, 'train_loss': 0.23567090966235632, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"model.save_pretrained(\"indobert-absa-model-3\")","metadata":{"execution":{"iopub.status.busy":"2024-06-25T15:20:02.493457Z","iopub.execute_input":"2024-06-25T15:20:02.493833Z","iopub.status.idle":"2024-06-25T15:20:03.034021Z","shell.execute_reply.started":"2024-06-25T15:20:02.493804Z","shell.execute_reply":"2024-06-25T15:20:03.03278Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"results = trainer.evaluate(tokenized_test_dataset)\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T16:36:24.106856Z","iopub.execute_input":"2024-06-25T16:36:24.107668Z","iopub.status.idle":"2024-06-25T16:36:31.761272Z","shell.execute_reply.started":"2024-06-25T16:36:24.107633Z","shell.execute_reply":"2024-06-25T16:36:31.76013Z"},"trusted":true},"execution_count":126,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='72' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [72/72 00:07]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.23042859137058258, 'eval_runtime': 7.6422, 'eval_samples_per_second': 150.742, 'eval_steps_per_second': 9.421, 'epoch': 3.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"test = Dataset.from_pandas(test_df)\ntest = test.map(preprocess_function, batched=True)\ntest = test.remove_columns([\"review\", \"rating\", \"variable\", \"value\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-25T15:24:55.98385Z","iopub.execute_input":"2024-06-25T15:24:55.984185Z","iopub.status.idle":"2024-06-25T15:24:56.953364Z","shell.execute_reply.started":"2024-06-25T15:24:55.984161Z","shell.execute_reply":"2024-06-25T15:24:56.952437Z"},"trusted":true},"execution_count":67,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1152 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bac9acecee834c19b4efdd6ab673d5ba"}},"metadata":{}}]},{"cell_type":"code","source":"import torch\n\n# Example test sample\ntest_aspect = \"barang\"\ntest_review = \"lumayan\"\n\n# Preprocess the test sample\ndef preprocess_single_sample(aspect, review):\n    combined_texts = aspect_token + aspect + review_token + review\n    print(len(combined_texts))\n    encoding = tokenizer(\n        combined_texts, \n        padding=\"max_length\", \n        truncation=True, \n        max_length=128,\n        return_tensors=\"pt\"\n    )\n    \n    return encoding\n\n# Preprocess the sample\nencoding = preprocess_single_sample(test_aspect, test_review)\n\n# Move inputs to the same device as the model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ninputs = {key: value.to(device) for key, value in encoding.items()}\nmodel.to(device)\n\n# Put model in evaluation mode\nmodel.eval()\n\n# Run inference\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# Get predicted label\nlogits = outputs.logits\n# print(logits.shape)\npredicted_class_id = torch.argmax(logits, dim=1).item()\n\n# Map class id to label (Assuming 0: Negative, 1: Neutral, 2: Positive)\nlabel_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\npredicted_label = label_map[predicted_class_id]\n\nprint(f\"Aspect: {test_aspect}\")\nprint(f\"Review: {test_review}\")\nprint(f\"Predicted Sentiment: {predicted_label}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-25T16:36:42.580679Z","iopub.execute_input":"2024-06-25T16:36:42.581872Z","iopub.status.idle":"2024-06-25T16:36:42.619543Z","shell.execute_reply.started":"2024-06-25T16:36:42.581837Z","shell.execute_reply":"2024-06-25T16:36:42.618521Z"},"trusted":true},"execution_count":127,"outputs":[{"name":"stdout","text":"29\nAspect: barang\nReview: lumayan\nPredicted Sentiment: Neutral\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}