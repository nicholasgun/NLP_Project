{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8781255,"sourceType":"datasetVersion","datasetId":5278420}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nicost312/sentiment-analysis-tokopedia?scriptVersionId=185608462\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport numpy as np\nimport pandas as pd\nimport re\n\nfrom datasets import load_from_disk\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n\nfrom sklearn.metrics import f1_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-26T16:45:10.555367Z","iopub.execute_input":"2024-06-26T16:45:10.556095Z","iopub.status.idle":"2024-06-26T16:45:42.201714Z","shell.execute_reply.started":"2024-06-26T16:45:10.556059Z","shell.execute_reply":"2024-06-26T16:45:42.200935Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"indolem/indobert-base-uncased\")\nreview_token = '[REVIEW]'\naspect_token = '[ASPECT]'\nspecial_tokens_dict = {'additional_special_tokens': [review_token, aspect_token]}\nnum_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n\n\ndef clean_text(texts):\n    cleaned_text = []\n    \n    for text in texts:\n        \n        text = text.lower()\n\n        text = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n\n        punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n        for p in punctuations:\n            text = text.replace(p,'') #Removing punctuations\n\n        emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n        text = emoji_pattern.sub(r'', text) #Removing emojis\n        cleaned_text.append(text)\n    \n    return cleaned_text\n\n# Preprocess function\ndef preprocess_function(examples):\n    combined_texts = [aspect_token + aspect + review_token + review for aspect, review in zip(examples[\"variable\"], examples[\"review\"])]\n    encoding =  tokenizer(\n        clean_text(examples[\"review\"]), \n        padding=\"max_length\", \n        truncation=True, \n        max_length=128\n    )\n    \n\n    labels_matrix = np.zeros((len(examples['review']), 3))\n    \n    for i, label in enumerate(examples[\"value\"]):\n        labels_matrix[i, int(label)] = 1\n\n        encoding[\"labels\"] = labels_matrix.tolist()\n  \n    return encoding\n\ntrain_df = pd.read_csv('/kaggle/input/bert-absa-dataset/out.csv').iloc[:4612, :]\ntest_df = pd.read_csv('/kaggle/input/bert-absa-dataset/out.csv').iloc[4612:, :]\n\n# Convert your data to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\ntokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\ntokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n\ntokenized_train_dataset = tokenized_train_dataset.remove_columns([\"review\", \"rating\", \"variable\", \"value\"])\ntokenized_test_dataset = tokenized_test_dataset.remove_columns([\"review\", \"rating\", \"variable\", \"value\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-26T16:45:42.203134Z","iopub.execute_input":"2024-06-26T16:45:42.203708Z","iopub.status.idle":"2024-06-26T16:45:46.616597Z","shell.execute_reply.started":"2024-06-26T16:45:42.203682Z","shell.execute_reply":"2024-06-26T16:45:46.615711Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9b5feddd37648c4ad02ff25715716f4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2a0a6fc26fa4b9188e8baff80b0f82a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/234k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9396ed9fb3f14a9cb1408d3c22de9c2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"360b761f00c24659a76ca9849a2a2e6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"353bae739ff04bbeb976d6cbd1092bc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4612 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c335eed8e7ea473d8640520d932f7087"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1152 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73a8f5053bce4f63a0937987412f54ff"}},"metadata":{}}]},{"cell_type":"code","source":"# Load pre-trained model with a classification head\nmodel = AutoModelForSequenceClassification.from_pretrained(\"indolem/indobert-base-uncased\", num_labels=3)\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2024-06-26T16:45:48.697448Z","iopub.execute_input":"2024-06-26T16:45:48.697899Z","iopub.status.idle":"2024-06-26T16:45:52.987119Z","shell.execute_reply.started":"2024-06-26T16:45:48.697852Z","shell.execute_reply":"2024-06-26T16:45:52.986182Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/445M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f83fd09ccbf2435a92cb40898e7104d2"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at indolem/indobert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"Embedding(31925, 768)"},"metadata":{}}]},{"cell_type":"code","source":"print(tokenized_train_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T16:45:53.946967Z","iopub.execute_input":"2024-06-26T16:45:53.947713Z","iopub.status.idle":"2024-06-26T16:45:53.952642Z","shell.execute_reply.started":"2024-06-26T16:45:53.947674Z","shell.execute_reply":"2024-06-26T16:45:53.951566Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n    num_rows: 4612\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\nfrom transformers import EvalPrediction\nimport torch\n    \ndef accuracy_metric(predictions, labels, threshold=0.5):\n    y_true = labels\n    \n#     print(predictions[5])\n    y_pred_indices = np.argmax(predictions, axis=1)\n    y_pred_one_hot = np.zeros_like(predictions)\n    y_pred_one_hot[np.arange(predictions.shape[0]), y_pred_indices] = 1\n    \n#     print(y_true[5])\n#     print(y_pred_one_hot[5])\n    accuracy = accuracy_score(y_true, y_pred_one_hot)\n    # return as dictionary\n    metrics = {'accuracy': accuracy}\n    return metrics\n\ndef compute_metrics(p: EvalPrediction):\n    preds = p.predictions[0] if isinstance(p.predictions, \n            tuple) else p.predictions\n    result = accuracy_metric(\n        predictions=preds, \n        labels=p.label_ids)\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-06-26T16:45:57.050619Z","iopub.execute_input":"2024-06-26T16:45:57.050967Z","iopub.status.idle":"2024-06-26T16:45:57.058753Z","shell.execute_reply.started":"2024-06-26T16:45:57.050939Z","shell.execute_reply":"2024-06-26T16:45:57.057655Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    load_best_model_at_end=True,\n    weight_decay=0.01,\n)\n\n# Define trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_test_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\n# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T16:46:14.678085Z","iopub.execute_input":"2024-06-26T16:46:14.678965Z","iopub.status.idle":"2024-06-26T16:50:20.871675Z","shell.execute_reply.started":"2024-06-26T16:46:14.67893Z","shell.execute_reply":"2024-06-26T16:50:20.870668Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111340029999989, max=1.0)â¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d381b732179b466cacc2c931bf1951b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240626_164620-kz74lq5w</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/tokped_sentiment_analysis/huggingface/runs/kz74lq5w' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/tokped_sentiment_analysis/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/tokped_sentiment_analysis/huggingface' target=\"_blank\">https://wandb.ai/tokped_sentiment_analysis/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/tokped_sentiment_analysis/huggingface/runs/kz74lq5w' target=\"_blank\">https://wandb.ai/tokped_sentiment_analysis/huggingface/runs/kz74lq5w</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='435' max='435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [435/435 03:33, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.249628</td>\n      <td>0.848090</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.237722</td>\n      <td>0.863715</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.232481</td>\n      <td>0.864583</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=435, training_loss=0.23683322599564477, metrics={'train_runtime': 244.3043, 'train_samples_per_second': 56.634, 'train_steps_per_second': 1.781, 'total_flos': 910109311921152.0, 'train_loss': 0.23683322599564477, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"model.save_pretrained(\"indobert-absa-model-final\")","metadata":{"execution":{"iopub.status.busy":"2024-06-25T17:08:23.822171Z","iopub.execute_input":"2024-06-25T17:08:23.822545Z","iopub.status.idle":"2024-06-25T17:08:24.539454Z","shell.execute_reply.started":"2024-06-25T17:08:23.822512Z","shell.execute_reply":"2024-06-25T17:08:24.538313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = trainer.evaluate(tokenized_test_dataset)\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T16:51:00.323049Z","iopub.execute_input":"2024-06-26T16:51:00.323604Z","iopub.status.idle":"2024-06-26T16:51:05.941264Z","shell.execute_reply.started":"2024-06-26T16:51:00.323569Z","shell.execute_reply":"2024-06-26T16:51:05.940207Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [36/36 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.23248052597045898, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 5.6028, 'eval_samples_per_second': 205.612, 'eval_steps_per_second': 6.425, 'epoch': 3.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\n# Example test sample\ntest_aspect = \"barang\"\ntest_review = \"lumayan\"\n\n# Preprocess the test sample\ndef preprocess_single_sample(aspect, review):\n    combined_texts = aspect_token + aspect + review_token + review\n    print(len(combined_texts))\n    encoding = tokenizer(\n        combined_texts, \n        padding=\"max_length\", \n        truncation=True, \n        max_length=128,\n        return_tensors=\"pt\"\n    )\n    \n    return encoding\n\n# Preprocess the sample\nencoding = preprocess_single_sample(test_aspect, test_review)\n\n# Move inputs to the same device as the model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ninputs = {key: value.to(device) for key, value in encoding.items()}\nmodel.to(device)\n\n# Put model in evaluation mode\nmodel.eval()\n\n# Run inference\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# Get predicted label\nlogits = outputs.logits\n# print(logits.shape)\npredicted_class_id = torch.argmax(logits, dim=1).item()\n\n# Map class id to label (Assuming 0: Negative, 1: Neutral, 2: Positive)\nlabel_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\npredicted_label = label_map[predicted_class_id]\n\nprint(f\"Aspect: {test_aspect}\")\nprint(f\"Review: {test_review}\")\nprint(f\"Predicted Sentiment: {predicted_label}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-25T17:08:17.833043Z","iopub.execute_input":"2024-06-25T17:08:17.833823Z","iopub.status.idle":"2024-06-25T17:08:17.871069Z","shell.execute_reply.started":"2024-06-25T17:08:17.833792Z","shell.execute_reply":"2024-06-25T17:08:17.870192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MULTILINGUAL","metadata":{}},{"cell_type":"code","source":"# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\nreview_token = '[REVIEW]'\naspect_token = '[ASPECT]'\nspecial_tokens_dict = {'additional_special_tokens': [review_token, aspect_token]}\nnum_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n\n\ndef clean_text(texts):\n    cleaned_text = []\n    \n    for text in texts:\n        \n        text = text.lower()\n\n        text = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n\n        punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n        for p in punctuations:\n            text = text.replace(p,'') #Removing punctuations\n\n        emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n        text = emoji_pattern.sub(r'', text) #Removing emojis\n        cleaned_text.append(text)\n    \n    return cleaned_text\n\n# Preprocess function\ndef preprocess_function(examples):\n    combined_texts = [aspect_token + aspect + review_token + review for aspect, review in zip(examples[\"variable\"], examples[\"review\"])]\n    encoding =  tokenizer(\n        clean_text(examples[\"review\"]), \n        padding=\"max_length\", \n        truncation=True, \n        max_length=128\n    )\n    \n\n    labels_matrix = np.zeros((len(examples['review']), 3))\n    \n    for i, label in enumerate(examples[\"value\"]):\n        labels_matrix[i, int(label)] = 1\n\n        encoding[\"labels\"] = labels_matrix.tolist()\n  \n    return encoding\n\ntrain_df = pd.read_csv('/kaggle/input/bert-absa-dataset/out.csv').iloc[:4612, :]\ntest_df = pd.read_csv('/kaggle/input/bert-absa-dataset/out.csv').iloc[4612:, :]\n\n# Convert your data to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\ntokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\ntokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n\ntokenized_train_dataset = tokenized_train_dataset.remove_columns([\"review\", \"rating\", \"variable\", \"value\"])\ntokenized_test_dataset = tokenized_test_dataset.remove_columns([\"review\", \"rating\", \"variable\", \"value\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-26T16:51:10.234893Z","iopub.execute_input":"2024-06-26T16:51:10.235561Z","iopub.status.idle":"2024-06-26T16:51:14.569339Z","shell.execute_reply.started":"2024-06-26T16:51:10.235528Z","shell.execute_reply":"2024-06-26T16:51:14.568134Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c23d3b2912dc4048a96bb08f9bef3c3b"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24e8a2d5767f4048bab00c9723dca231"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d146aac44cb54c44a2700966b454a2e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58009c9d9f024c538445d34cf600547d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4612 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f330c9e8254c421bb69be78a4d8f9975"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1152 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e193d15a53464393bb2bc2cfad74216f"}},"metadata":{}}]},{"cell_type":"code","source":"# Load pre-trained model with a classification head\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=3)\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2024-06-26T16:51:16.088863Z","iopub.execute_input":"2024-06-26T16:51:16.089659Z","iopub.status.idle":"2024-06-26T16:51:43.266659Z","shell.execute_reply.started":"2024-06-26T16:51:16.089626Z","shell.execute_reply":"2024-06-26T16:51:43.265386Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"922b3780923b4d4db4a97696e8d9852e"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Embedding(119549, 768)"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    load_best_model_at_end=True,\n    weight_decay=0.01,\n)\n\n# Define trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_test_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\n# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:33:32.864676Z","iopub.execute_input":"2024-06-26T06:33:32.865065Z","iopub.status.idle":"2024-06-26T06:39:12.67008Z","shell.execute_reply.started":"2024-06-26T06:33:32.865036Z","shell.execute_reply":"2024-06-26T06:39:12.669219Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240626_063412-7p4xxxqb</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/tokped_sentiment_analysis/huggingface/runs/7p4xxxqb' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/tokped_sentiment_analysis/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/tokped_sentiment_analysis/huggingface' target=\"_blank\">https://wandb.ai/tokped_sentiment_analysis/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/tokped_sentiment_analysis/huggingface/runs/7p4xxxqb' target=\"_blank\">https://wandb.ai/tokped_sentiment_analysis/huggingface/runs/7p4xxxqb</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='435' max='435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [435/435 04:38, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.276519</td>\n      <td>0.820312</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.245479</td>\n      <td>0.848958</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.249207</td>\n      <td>0.853299</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=435, training_loss=0.23655486709770115, metrics={'train_runtime': 337.3014, 'train_samples_per_second': 41.02, 'train_steps_per_second': 1.29, 'total_flos': 910109311921152.0, 'train_loss': 0.23655486709770115, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"results = trainer.evaluate(tokenized_test_dataset)\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:39:52.11352Z","iopub.execute_input":"2024-06-26T06:39:52.114154Z","iopub.status.idle":"2024-06-26T06:39:58.824998Z","shell.execute_reply.started":"2024-06-26T06:39:52.114121Z","shell.execute_reply":"2024-06-26T06:39:58.82394Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [36/36 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.2454787641763687, 'eval_accuracy': 0.8489583333333334, 'eval_runtime': 6.6989, 'eval_samples_per_second': 171.969, 'eval_steps_per_second': 5.374, 'epoch': 3.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save_pretrained(\"multilingual-absa-model-final\")","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:40:21.479377Z","iopub.execute_input":"2024-06-26T06:40:21.479834Z","iopub.status.idle":"2024-06-26T06:40:23.050261Z","shell.execute_reply.started":"2024-06-26T06:40:21.479794Z","shell.execute_reply":"2024-06-26T06:40:23.049189Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Indo ROBERTA","metadata":{}},{"cell_type":"code","source":"# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"cahya/roberta-base-indonesian-522M\")\nreview_token = '[REVIEW]'\naspect_token = '[ASPECT]'\nspecial_tokens_dict = {'additional_special_tokens': [review_token, aspect_token]}\nnum_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n\n\ndef clean_text(texts):\n    cleaned_text = []\n    \n    for text in texts:\n        \n        text = text.lower()\n\n        text = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n\n        punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n        for p in punctuations:\n            text = text.replace(p,'') #Removing punctuations\n\n        emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n        text = emoji_pattern.sub(r'', text) #Removing emojis\n        cleaned_text.append(text)\n    \n    return cleaned_text\n\n# Preprocess function\ndef preprocess_function(examples):\n    combined_texts = [aspect_token + aspect + review_token + review for aspect, review in zip(examples[\"variable\"], examples[\"review\"])]\n    encoding =  tokenizer(\n        clean_text(examples[\"review\"]), \n        padding=\"max_length\", \n        truncation=True, \n        max_length=128\n    )\n    \n\n    labels_matrix = np.zeros((len(examples['review']), 3))\n    \n    for i, label in enumerate(examples[\"value\"]):\n        labels_matrix[i, int(label)] = 1\n\n        encoding[\"labels\"] = labels_matrix.tolist()\n  \n    return encoding\n\ntrain_df = pd.read_csv('/kaggle/input/bert-absa-dataset/out.csv').iloc[:4612, :]\ntest_df = pd.read_csv('/kaggle/input/bert-absa-dataset/out.csv').iloc[4612:, :]\n\n# Convert your data to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\ntokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\ntokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n\ntokenized_train_dataset = tokenized_train_dataset.remove_columns([\"review\", \"rating\", \"variable\", \"value\"])\ntokenized_test_dataset = tokenized_test_dataset.remove_columns([\"review\", \"rating\", \"variable\", \"value\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:40:56.597298Z","iopub.execute_input":"2024-06-26T06:40:56.598147Z","iopub.status.idle":"2024-06-26T06:41:01.609839Z","shell.execute_reply.started":"2024-06-26T06:40:56.598117Z","shell.execute_reply":"2024-06-26T06:41:01.60858Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/150 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"303c90f1f2ce4d14a34c0909479b3222"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"110f3a5d400b46b49b1dc81be509a9d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/926k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49d68d30fbf043b8b5cf3d74926e2edf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/468k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce525a32cfce41b1b9a6e95302d61ee8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4612 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed969c4d31244779ae7328d98c510607"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1152 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fccc0199c9944f5182ffb70744e4bfa3"}},"metadata":{}}]},{"cell_type":"code","source":"# Load pre-trained model with a classification head\nmodel = AutoModelForSequenceClassification.from_pretrained(\"cahya/roberta-base-indonesian-522M\", num_labels=3)\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:41:30.098115Z","iopub.execute_input":"2024-06-26T06:41:30.098494Z","iopub.status.idle":"2024-06-26T06:41:51.844853Z","shell.execute_reply.started":"2024-06-26T06:41:30.098465Z","shell.execute_reply":"2024-06-26T06:41:51.843549Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/507M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b58dd4b8a8264dcabda31c3501721ffb"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cahya/roberta-base-indonesian-522M and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Embedding(52002, 768)"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    load_best_model_at_end=True,\n    weight_decay=0.01,\n)\n\n# Define trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_test_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\n# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:42:00.483122Z","iopub.execute_input":"2024-06-26T06:42:00.483557Z","iopub.status.idle":"2024-06-26T06:45:50.526215Z","shell.execute_reply.started":"2024-06-26T06:42:00.483514Z","shell.execute_reply":"2024-06-26T06:45:50.525096Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='435' max='435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [435/435 03:48, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.258609</td>\n      <td>0.843750</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.243934</td>\n      <td>0.845486</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.246464</td>\n      <td>0.845486</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=435, training_loss=0.20075331063106142, metrics={'train_runtime': 229.1889, 'train_samples_per_second': 60.369, 'train_steps_per_second': 1.898, 'total_flos': 910109311921152.0, 'train_loss': 0.20075331063106142, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"results = trainer.evaluate(tokenized_test_dataset)\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:45:50.527944Z","iopub.execute_input":"2024-06-26T06:45:50.528312Z","iopub.status.idle":"2024-06-26T06:45:56.350346Z","shell.execute_reply.started":"2024-06-26T06:45:50.528277Z","shell.execute_reply":"2024-06-26T06:45:56.349091Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [36/36 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.24393394589424133, 'eval_accuracy': 0.8454861111111112, 'eval_runtime': 5.8078, 'eval_samples_per_second': 198.355, 'eval_steps_per_second': 6.199, 'epoch': 3.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save_pretrained(\"roberta-absa-model-final\")","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:46:13.104433Z","iopub.execute_input":"2024-06-26T06:46:13.104868Z","iopub.status.idle":"2024-06-26T06:46:13.955553Z","shell.execute_reply.started":"2024-06-26T06:46:13.104836Z","shell.execute_reply":"2024-06-26T06:46:13.954423Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}